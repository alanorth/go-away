# 2025-04-22: Based on examples/generic.yml

# Example cmdline (forward requests from upstream to port :8080)
# $ go-away --bind :8923 --backend site.example.com=http://site:4000 --policy examples/generic.yml --challenge-template anubis



# Define networks to be used later below
networks:
  # Networks will get included from snippets

challenges:
  # Challenges will get included from snippets

conditions:
  # Conditions will get replaced on rules AST when found as ($condition-name)

  # Conditions will get included from snippets

  is-static-asset:
    - 'path.startsWith("/assets/")'
    # Most of these will be served from nginx directly but I will leave them here
    # just in case.
    - 'path.matches("\\.(avif|css|gif|js|json|jpeg|jpg|manifest|mjs|mp4|png|svg|ttf|wasm|webm|webp|woff|woff2)$")'

  is-suspicious-crawler:
    - 'userAgent.contains("Trident/")'
    # Old IE browsers
    - 'userAgent.matches("MSIE ([2-9]|10|11)\\.")'
    # Old Linux browsers
    - 'userAgent.matches("Linux i[63]86") || userAgent.matches("FreeBSD i[63]86")'
    # Old Windows browsers
    - 'userAgent.matches("Windows (3|95|98|CE)") || userAgent.matches("Windows NT [1-5]\\.")'
    # Old mobile browsers
    - 'userAgent.matches("Android [1-5]\\.") || userAgent.matches("(iPad|iPhone) OS [1-9]_")'
    #- 'userAgent.matches("Gecko/(201[0-9]|200[0-9])")'
    - 'userAgent.matches("^Mozilla/[1-4]")'
    # Old Chrome (versions x, xx, and 10x/11x/12x). Version 129 was released in 2024-09
    - 'userAgent.matches("Chrome/\\d{1,2}\\.") || userAgent.matches("Chrome/1[012]\\d\\.")'
    # Old Firefox (versions x, xx, and 10x/11x/12x), except version 128 ESR
    - '(userAgent.matches("Firefox/\\d{1,2}\\.") || userAgent.matches("Firefox/1[012]\\d\\.")) && !userAgent.matches("Firefox/128\\.")'

  # DSpace search and browse pages are heavy
  is-heavy-resource:
    # DSpace 7+ top-level browse: https://demo.dspace.org/browse/dateissued
    # DSpace 8+ community/collection browse: https://demo.dspace.org/collections/6a259fbf-b49b-47c3-9122-748930752229/browse/dateissued
    - 'path.contains("/browse/")'
    # DSpace 7+ top-level search: https://demo.dspace.org/search?query=simmons
    - 'path.contains("/search") && ("query" in query && query.query != "")'
    # DSpace 7+ top-level search facets: https://demo.dspace.org/search?f.author=Simmons%2C%20Cameron%2Cequals&f.author=Ha%2C%20MT%2Cequals&spc.page=1
    # DSpace 9+ community/collection search: https://demo.dspace.org/communities/0958c910-2037-42a9-81c7-dca80e3892b4/search?f.author=Simmons%2C%20Cameron%2Cequals&spc.page=1
    # Path must contain "search" AND have an spc.page parameter. This seems to
    # always be present for search queries and facet searches.
    - 'path.contains("/search") && "spc.page" in query'

# Rules are checked sequentially in order, from top to bottom
rules:
  - name: allow-well-known-resources
    conditions:
      - '($is-well-known-asset)'
    action: pass

  - name: allow-static-resources
    conditions:
      - '($is-static-asset)'
    action: pass

  - name: desired-crawlers
    conditions:
      - *is-bot-googlebot
      - *is-bot-bingbot
      - *is-bot-duckduckbot
      - *is-bot-kagibot
      - *is-bot-qwantbot
      - *is-bot-yandexbot
      - *is-bot-uptimerobot
      - *is-bot-opera-mini
      - *is-bot-facebook-share
      - *is-bot-chatgpt-user
    action: pass

  # These are defined in examples/snippets-ilri/networks-other.yml
  - name: undesired-networks
    conditions:
      - 'remoteAddress.network("gptbot")'
      - 'remoteAddress.network("aws-cloud")'
      - 'remoteAddress.network("azure-cloud")'
      - 'remoteAddress.network("cloudflare")'
      - 'remoteAddress.network("digitalocean")'
      - 'remoteAddress.network("google-cloud")'
      - 'remoteAddress.network("linode")'
      - 'remoteAddress.network("oracle-cloud")'
      - 'remoteAddress.network("vultr")'
      # Other datacenters
      - 'remoteAddress.network("datacenters")'
      # Chinese networks hitting /search hard
      - 'remoteAddress.network("chinese-crawlers")'
    action: drop

  - name: undesired-crawlers
    conditions:
      - '($is-headless-chromium)'
      - 'userAgent.startsWith("Lightpanda/")'
      - 'userAgent.startsWith("masscan/")'
      # Typo'd opera botnet
      - 'userAgent.matches("^Opera/[0-9.]+\\.\\(")'
      # AI bullshit stuff, they do not respect robots.txt even while they read it
      # TikTok Bytedance AI training
      - 'userAgent.contains("Bytedance") || userAgent.contains("Bytespider") || userAgent.contains("TikTokSpider")'
      # Meta AI training; The Meta-ExternalAgent crawler crawls the web for use cases such as training AI models or improving products by indexing content directly.
      - 'userAgent.contains("meta-externalagent/") || userAgent.contains("meta-externalfetcher/") || userAgent.contains("FacebookBot")'
      # Anthropic AI training and usage
      - 'userAgent.contains("ClaudeBot") || userAgent.contains("Claude-User")|| userAgent.contains("Claude-SearchBot")'
      # Common Crawl AI crawlers
      - 'userAgent.contains("CCBot")'
      # ChatGPT AI crawlers https://platform.openai.com/docs/bots
      - 'userAgent.contains("GPTBot") || userAgent.contains("OAI-SearchBot") || userAgent.contains("ChatGPT-User")'
      # Other AI crawlers
      - 'userAgent.contains("Amazonbot") || userAgent.contains("Google-Extended") || userAgent.contains("PanguBot") || userAgent.contains("AI2Bot") || userAgent.contains("Diffbot") || userAgent.contains("cohere-training-data-crawler") || userAgent.contains("Applebot-Extended")'
      # SEO / Ads and marketing
      - 'userAgent.contains("BLEXBot")'
      - 'userAgent.contains("Scrapy")'
    action: drop

  - name: unknown-crawlers
    conditions:
      # No user agent set
      - 'userAgent == ""'
    action: deny

  - name: suspicious-crawlers
    conditions:
      - '($is-suspicious-crawler)'
      - '($is-unrealistic-browser)'
    action: challenge
    settings:
      challenges: [js-pow-sha256]

  ###
  # In theory "real" users will start being challenged here.
  ###

  # TEST
  #- name: always-pow
  #  conditions:
  #    - 'path == "/items/bef99a30-aaeb-4ccd-af55-9adbe6e68f89"'
  #  action: challenge
  #  settings:
  #    challenges: [js-pow-sha256]

  - name: heavy-operations
    conditions: ['($is-heavy-resource)']
    action: challenge
    settings:
      challenges: [js-pow-sha256]

  - name: homesite
    conditions:
      - 'path == "/" || path == "/home"'
      # Allow loading of DSpace objects without challenges
      - 'path.startsWith("/bitstream/")'
      - 'path.startsWith("/bitstreams/")'
      - 'path.startsWith("/collections/")'
      - 'path.startsWith("/communities/")'
      - 'path.startsWith("/entities/")'
      - 'path.startsWith("/handle/")'
      - 'path.startsWith("/items/")'
    action: pass

  #- name: plaintext-browser
  #  action: challenge
  #  settings:
  #    challenges: [meta-refresh, cookie]
  #  conditions:
  #    - 'userAgent.startsWith("Lynx/")'

  # Comment this rule out to not challenge tool-like user agents
  #- name: standard-tools
  #  action: challenge
  #  settings:
  #    challenges: [cookie]
  #  conditions:
  #    - '($is-tool-ua)'
  #    - '!($is-generic-browser)'

  #- name: standard-browser
  #  action: challenge
  #  settings:
  #    challenges: [preload-link, meta-refresh, resource-load, js-refresh, js-pow-sha256]
  #  conditions:
  #    - '($is-generic-browser)'

# If end of rules is reached, default is PASS
